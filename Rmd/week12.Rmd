---
title: "week12.Rmd"
author: "Joy Zhou"
date: "2023-04-14"
output: html_document
---

### Script Settings and Resources
```{r, message=F}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(RedditExtractoR)
library(tidyverse)
library(rvest)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(parallel)
library(doParallel)
library(topicmodels)
library(tidytext)
library(wordcloud)
library(stats)
```

### Data Import and Cleaning
```{r}

#io_psych_url <- find_thread_urls(
  #subreddit = "IOPsychology",
  #period = "year",
  #sort_by = "new"
#)

#io_psych_content <- get_thread_content(io_psych_url$url)

#title <- io_psych_content$threads$title
#upvotes <- io_psych_content$threads$upvotes

#week12_tbl <- tibble(title, upvotes)

#write.csv(week12_tbl, "../data/week12_tbl.csv", row.names = F)

week12_tbl <- read.csv("../data/week12_tbl.csv")

io_corpus_original <- VCorpus(VectorSource(week12_tbl[,1]))

io_stem <- c("io", "i/o")
psych_stem <- c("psychology", "psychologist", "psychologists", "psych")
##to_lower needs to be after contraction as the latter capitalizes the first letter in a sentence
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_replace_all), pattern = "’", replacement = "'") %>%
  tm_map(content_transformer(str_replace_all), pattern = "-|/", replacement = " ") %>%
  tm_map(content_transformer(str_remove), pattern = "‘|“|”") %>%
  tm_map(content_transformer(replace_abbreviation)) %>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(removeWords, c(io_stem, psych_stem, "iopsychology", "riopsychology")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

compare_them <- function(corpus_1, corpus_2){
  sample_num <- sample(nrow(week12_tbl), 1)
  compare <- list(corpus_1[[sample_num]]$content, corpus_2[[sample_num]]$content)
  return(compare)
}

compare_them(io_corpus, io_corpus_original)

bi_token <- function(x){
  NGramTokenizer(x, Weka_control(min = 1, max = 2))
}

io_dtm <- DocumentTermMatrix(
  io_corpus, 
  control = list(tokenizer = bi_token)
)

io_slim_dtm <- removeSparseTerms(io_dtm, 0.996)

tokenCounts <- apply(io_slim_dtm, 1, sum)
io_slim_dtm_complete <- io_slim_dtm[tokenCounts > 0, ]
```

### Analysis
```{r}
cluster <- makeCluster(7)
registerDoParallel(cluster)

io_tuning <- FindTopicsNumber(
  io_slim_dtm_complete,
  topics = seq(3,15,3),
  metrics = c("Griffiths2004",
              "CaoJuan2009",
              "Arun2010",
              "Deveaud2014"),
  verbose = T
)
FindTopicsNumber_plot(io_tuning)

stopCluster(cluster)
registerDoSEQ()

lda_results <- LDA(io_slim_dtm_complete, 9)

lda_betas <- tidy(lda_results, matrix = "beta")
lda_gammas <- tidy(lda_results, matrix = "gamma")

lda_betas %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  arrange(topic) %>%
  view()

lda_gammas_selected <- lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(doc_id = as.numeric(document),
         probability = gamma) %>%
  arrange(document) %>%
  select(-document, -gamma)

doc_id <- c(1:nrow(week12_tbl))
original <- week12_tbl$title

doc_title_tibble <- tibble(doc_id, original)

topics_tbl <- doc_title_tibble %>%
  left_join(lda_gammas_selected, by = "doc_id")

slim_dtm_df <- as_tibble(as.matrix(io_slim_dtm))

wordcounts <- colSums(slim_dtm_df)
wordnames <- names(slim_dtm_df)

wordcloud(wordnames, wordcounts, max.words = 50, scale = c(2, .5))

week12_tbl_num <- week12_tbl %>%
  mutate(doc_id = as.numeric(1:nrow(week12_tbl)))

final_tbl <- topics_tbl %>%
  left_join(week12_tbl_num, by = "doc_id")

oneway.test(upvotes ~ topic, data = final_tbl)
```




