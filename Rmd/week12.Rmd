---
title: "week12.Rmd"
author: "Joy Zhou"
date: "2023-04-14"
output: html_document
---

### Script Settings and Resources
```{r setup}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(rvest)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(parallel)
library(doParallel)
library(topicmodels)
library(tidytext)
library(wordcloud)
library(stats)
```

### Data Import and Cleaning
```{r}
io_psych_html <- "https://old.reddit.com/r/IOPsychology/top/?t=year"

num_pages <- 30
title <- c()
upvotes <- c()

# Loop through each page and scrape the relevant information
for (i in 1:num_pages) {
  if (i == 1){
  # Scrape the HTML content of the current page
  page_html <- read_html(io_psych_html)
    
  # Extract the titles and scores of the top posts using CSS selectors
  titles <- page_html %>% 
      ## To screen out advertisement posts, I only selected the posts that are "submitted" by users rather than "promoted"
      html_elements(xpath = '//div[p[text() = "submitted "]]') %>%
      ## Selected the title class where the titles are stored in
      html_elements(xpath = 'p[@class = "title"]/a') %>%
      ## Extracted the title text from the class
      html_text()
    
   upvote <- page_html %>%
  ## Same as above, # To screen out advertisement posts, I only selected the posts that are "submitted" by users rather than "promoted"
     html_elements(xpath = '//div[p[text() = "submitted "]]') %>%
  ## Went up to the parent (entry unvoted) of the div class that "submitted" was stored in (top matter), and then selected the preceding sibling (midcol unvoted) of the parent  
     html_elements(xpath = '..//preceding-sibling::div[@class = "midcol unvoted"]') %>%
  ## Selected the upvotes from the midcol unvoted div class
     html_elements(xpath = 'div[@class = "score unvoted"]') %>%
  ## Extracted the upvotes characters from the score unvoted class
     html_text() %>%
  ## Converted them into numeric values for later analyses
     as.numeric() %>%
  ## Replace NA with 0 
     replace_na(0)
   
  # Append the scraped data to the respective lists
  title <- c(title, titles)
  upvotes <- c(upvotes, upvote)
    
    # Extract the "after" ID from the last post on the page
    after_id <- page_html %>%
      html_elements(xpath = '..//div[@data-fullname]') %>%
      html_attr("data-fullname") %>%
      last()
    
    Sys.sleep(3)
  } else {
  page_url <- paste0(io_psych_html, "&count=", (i-1)*25, "&after=", after_id)
  
  # Scrape the HTML content of the current page
  page_html <- read_html(page_url)
  
  # Extract the titles and scores of the top posts using CSS selectors
  titles <- page_html %>% 
    ## To screen out advertisement posts, I only selected the posts that are "submitted" by users rather than "promoted"
    html_elements(xpath = '//div[p[text() = "submitted "]]') %>%
    ## Selected the title class where the titles are stored in
    html_elements(xpath = 'p[@class = "title"]/a') %>%
    ## Extracted the title text from the class
    html_text()
  
  upvote <- page_html %>%
  ## Same as above, # To screen out advertisement posts, I only selected the posts that are "submitted" by users rather than "promoted"
     html_elements(xpath = '//div[p[text() = "submitted "]]') %>%
  ## Went up to the parent (entry unvoted) of the div class that "submitted" was stored in (top matter), and then selected the preceding sibling (midcol unvoted) of the parent  
     html_elements(xpath = '..//preceding-sibling::div[@class = "midcol unvoted"]') %>%
  ## Selected the upvotes from the midcol unvoted div class
     html_elements(xpath = 'div[@class = "score unvoted"]') %>%
  ## Extracted the upvotes characters from the score unvoted class
     html_text() %>%
  ## Converted them into numeric values for later analyses
     as.numeric() %>%
  ## Replace NA with 0 
     replace_na(0)
     
  # Append the scraped data to the respective lists
  title <- c(title, titles)
  upvotes <- c(upvotes, upvote)
  
  after_id <- page_html %>%
    html_elements(xpath = '..//div[@data-fullname]') %>%
    html_attr("data-fullname") %>%
    last() 
  
  Sys.sleep(3)
  }
}

week12_tbl <- tibble(title, upvotes)
write.csv(week12_tbl, "../data/week12_tbl.csv", row.names = F)

week12_tbl <- read.csv("../data/week12_tbl.csv")

io_corpus_original <- VCorpus(VectorSource(week12_tbl[,1]))

io_stem <- c("io", "i/o")
psych_stem <- c("psychology", "psychologist", "psychologists", "psych")
##to_lower needs to be after contraction as the latter capitalizes the first letter in a sentence
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_replace_all), pattern = "â€™", replacement = "'") %>%
  tm_map(content_transformer(replace_abbreviation)) %>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(removeWords, c(io_stem, psych_stem)) %>%
  tm_map(stripWhitespace) %>%
  lemmatize_words() 

compare_them <- function(corpus_1, corpus_2){
  sample_num <- sample(nrow(week12_tbl), 1)
  compare <- list(corpus_1[[sample_num]]$content, corpus_2[[sample_num]]$content)
  return(compare)
}

compare_them(io_corpus, io_corpus_original)

bi_token <- function(x){
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}

io_dtm <- DocumentTermMatrix(
  io_corpus, 
  control = list(tokenizer = bi_token)
)

io_slim_dtm1 <- removeSparseTerms(io_dtm, 0.998)

tokenCounts <- apply(io_slim_dtm1, 1, sum)
io_slim_dtm <- io_slim_dtm1[tokenCounts > 0, ]
```

### Analysis
```{r}
cluster <- makeCluster(7)
registerDoParallel(cluster)

io_tuning <- FindTopicsNumber(
  io_slim_dtm,
  topics = seq(3,15,3),
  metrics = c("Griffiths2004",
              "CaoJuan2009",
              "Arun2010",
              "Deveaud2014"),
  verbose = T
)
FindTopicsNumber_plot(io_tuning)

stopCluster(cluster)
registerDoSEQ()

lda_results <- LDA(io_slim_dtm, 9)

lda_betas <- tidy(lda_results, matrix = "beta")
lda_gammas <- tidy(lda_results, matrix = "gamma")

lda_betas %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic) %>%
  view()

lda_gammas_selected <- lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(doc_id = as.numeric(document),
         probability = gamma) %>%
  arrange(document) %>%
  select(-document, -gamma)

doc_id <- c(1:nrow(week12_tbl))
original <- week12_tbl$title

doc_title_tibble <- tibble(doc_id, original)

topics_tbl <- doc_title_tibble %>%
  left_join(lda_gammas_selected, by = "doc_id")

slim_dtm_df <- as_tibble(as.matrix(io_slim_dtm))

wordcounts <- colSums(slim_dtm_df)
wordnames <- names(slim_dtm_df)

wordcloud(wordnames, wordcounts, max.words = 50, scale = c(2, .5))

week12_tbl_num <- week12_tbl %>%
  mutate(doc_id = as.numeric(1:nrow(week12_tbl)))

final_tbl <- topics_tbl %>%
  left_join(week12_tbl_num, by = "doc_id")

oneway.test(upvotes ~ topic, data = final_tbl)
```




